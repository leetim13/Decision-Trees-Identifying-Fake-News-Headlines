{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees: Identifying Fake vs Real News Headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this mini project is to use the `scikit-learn` decision tree classifier to classify real vs. fake news headlines. We will use a dataset of 1298 “fake news” headlines (which mostly include headlines of articles classified as biased, etc.) and 1968 “real” news headlines, where the “fake news” headlines are from https://www.kaggle.com/mrisdal/fake-news/data and “real news”headlines are from https://www.kaggle.com/therohk/million-headlines. The data were cleaned by removing words from titles not part of the headlines, removing special characters and restricting real news headlines after October 2016 using the word ”trump”. The cleaned data are available as `clean_real.txt`\n",
    "and `clean_fake.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import random\n",
    "import math\n",
    "import graphviz\n",
    "from collections import Counter\n",
    "\n",
    "TRAINING_PORPORTION = 0.7\n",
    "VALIDATION_PORPORTION = 0.15\n",
    "TEST_PORPORTION = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `load_data` which loads the data, preprocesses it using a vectorizer and splits the entire dataset randomly into 70% training, 15% validation, and 15% test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(vectorizer):\n",
    "    labelled_data = read_and_shuffle_labelled_data()\n",
    "  \n",
    "    headlines = [] #All headlines from tuple in list\n",
    "    for x in labelled_data:\n",
    "        headlines.append(x[0])\n",
    "    document_term_matrix = vectorizer.fit_transform(headlines) #fit and transform into document term matrix\n",
    "\n",
    "    labels = [] #All corresponding labels from tuple in list\n",
    "    for x in labelled_data:\n",
    "        labels.append(x[1])\n",
    "\n",
    "    rows = len(labelled_data) \n",
    "    endIndex_training = int(TRAINING_PORPORTION * rows) \n",
    "    startIndex_validation = endIndex_training \n",
    "    endIndex_validation = endIndex_training + int(VALIDATION_PORPORTION * rows) \n",
    "    startIndex_testing = int(TEST_PORPORTION * rows) + endIndex_training\n",
    "    \n",
    "    training_data = document_term_matrix[:endIndex_training]\n",
    "    validation_data = document_term_matrix[startIndex_validation:endIndex_validation]\n",
    "    test_data = document_term_matrix[startIndex_testing:]\n",
    "                                                             \n",
    "    train_labels = labels[:endIndex_training] \n",
    "    valid_labels = labels[startIndex_validation:endIndex_validation]\n",
    "    test_labels =  labels[startIndex_testing:]\n",
    "\n",
    "    return (training_data, train_labels), (validation_data, valid_labels), (test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `read_and_shuffle_labelled_data` which read and shuffle data into [(headline, label)] format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_shuffle_labelled_data():\n",
    "    \"\"\"\n",
    "    Read and shuffle data into [(headline, label)] format.\n",
    "    \"\"\"\n",
    "    with open(\"clean_fake.txt\") as fake_data_file:    \n",
    "        fake_data = fake_data_file.readlines()    \n",
    "    fake_data = [x.strip() for x in fake_data]\n",
    "    \n",
    "    fake_data_labelled = []\n",
    "    for data in fake_data:\n",
    "        fake_data_labelled.append((data, \"fake_news\"))\n",
    "\n",
    "    with open(\"clean_real.txt\") as real_data_file:    \n",
    "        real_data = real_data_file.readlines()    \n",
    "    real_data = [x.strip() for x in real_data]\n",
    "    \n",
    "    real_data_labelled = []\n",
    "    for data in real_data:\n",
    "        real_data_labelled.append((data, \"real_news\"))\n",
    "    \n",
    "    all_data = []\n",
    "    all_data.extend(real_data_labelled)\n",
    "    all_data.extend(fake_data_labelled)\n",
    "\n",
    "    random.shuffle(all_data)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `select_model` which trains the decision tree classifier using 5 different values of `max_depth`, as well as two different split criteria (information gain and Gini coefficient), evaluates the performance of each one on the validation set, and prints\n",
    "the resulting accuracies of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(training_data, training_labels, validation_data, validation_labels):\n",
    "    # Select best model/decision tree with highest validaiton score    \n",
    "    best_model = None #Initialize best model/tree\n",
    "    best_score = 0 #Initialize best score\n",
    "    \n",
    "    for default_criterion in [\"entropy\", \"gini\"]: \n",
    "        for max_depth in range(1, 6): #five different values from max depth {1,2,3,4,5}\n",
    "            model = DecisionTreeClassifier(criterion=default_criterion, max_depth=max_depth)\n",
    "            model.fit(training_data, training_labels)  #train and fit model on the validation data\n",
    "            score = evaluate_training_model(model, validation_data, validation_labels) #calls helper to evaluate highest score\n",
    "            print(\"Tree with depth \"  + str(max_depth) + \" and criteria = \" + str(default_criterion)+\n",
    "                  \" : validation score = \" + str(score))\n",
    "            if score > best_score: #update best score \n",
    "                best_score = score\n",
    "                best_model = model\n",
    "                       \n",
    "    print(\"\\n\"+ \"Best model is:\")\n",
    "    print(best_model)\n",
    "    \n",
    "    print(\"\\n\"+ \"Best score is:\")\n",
    "    print(best_score)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `evaluate_training_model` to evaluate the performance of each model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_training_model(training_model, validation_data, validation_labels): \n",
    "    model_predictions = training_model.predict(validation_data)\n",
    "    correct_count = 0 #Initialize counter of number of correct predictions\n",
    "    total_count = len(validation_labels) #Number of total counts\n",
    "    \n",
    "    for i in range(total_count):\n",
    "        if model_predictions[i] == validation_labels[i]:\n",
    "            correct_count += 1\n",
    "    score = correct_count / total_count\n",
    "\n",
    "    return score #proportion of correct_count / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `visualize_best_tree` to extract and visualize the first two layers of the tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_best_tree(training_model, tree_name):\n",
    "    export_graphviz(training_model, out_file=tree_name+\".dot\", max_depth=2,\n",
    "                    class_names=training_model.classes_,\n",
    "                    feature_names=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `log_entropy` function, i.e., compute $n * \\log(n)$ since entropy $H(x) = -\\Sigma(P_n*log(P_n))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_entropy(n):\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    return n * math.log(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `compute_entropy` function to compute entropy of given labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(labels_list):\n",
    "    num_total = len(labels_list)\n",
    "    num_fake = 0\n",
    "    num_real = 0 \n",
    "    for x in labels_list:\n",
    "        if x == \"fake_news\":\n",
    "            num_fake +=1\n",
    "        else:\n",
    "            num_real +=1\n",
    "            \n",
    "    total_fake = log_entropy(num_fake / num_total)\n",
    "    total_real = log_entropy(num_real / num_total)\n",
    "    total = -(total_fake + total_real) #By entropy formula H(x) = -plog(p) - qlog(q)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `compute_entropy_for_specific_vocab` function to compute entropy for specific user-defined vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy_for_specific_vocab(document_term_matrix, labels, vocabulary, specific_vocab):\n",
    "    # Column of the word in the document matrix\n",
    "    array_of_specific_vocab = document_term_matrix.getcol(vocabulary[specific_vocab])\n",
    "    word_columns = array_of_specific_vocab.toarray().flatten()\n",
    "\n",
    "    present_indices = {0: (1 - word_columns).nonzero()[0],  #index of vocab not in column\n",
    "        1: word_columns.nonzero()[0]}  # index of vocab in column\n",
    "\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    presented_labels_count = Counter( #store counts for corresponding fake and real news labels\n",
    "            (presence, labels[ind]) for presence in present_indices for ind in present_indices[presence])\n",
    "\n",
    "    probability_presented_labels = { #convert presented_labels_count into corresponding probabilites 0 <=p<=1\n",
    "        x: (presented_labels_count[x] / num_labels) \n",
    "        for x in presented_labels_count}\n",
    "\n",
    "    probability_presented_vocab = { # Probabilities of presented vocab words\n",
    "        presence: len(present_indices[presence]) / num_labels\n",
    "        for presence in present_indices}\n",
    "    \n",
    "\n",
    "    cond_probability_label_given_presented_vocab = { # Conditional probabilities of label given presented vocab word\n",
    "        (presence, label): probability_presented_labels[(presence, label)] /\n",
    "                           probability_presented_vocab[presence]\n",
    "        for (presence, label) in probability_presented_labels}\n",
    "    \n",
    "    return -1 * sum( #compute entropy using formula P(T) * nlogn(P(T|X)) \n",
    "        probability_presented_vocab[presence] * log_entropy(cond_probability_label_given_presented_vocab[(presence, label)])\n",
    "        for (presence, label) in cond_probability_label_given_presented_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `compute_information_gain` which computes the information gain of a split on the training data. That is, compute $I(Y, x_i)$, where $Y$ is the random variable signifying whether the headline is real or fake, and $x_i$ is the keyword chosen for the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_information_gain(document_term_matrix, headline_labels, vectorized_vocabulary, specific_vocab):\n",
    "    #Compute info gain by formula: Gain(T,X) = Entropy(T) - Entropy(T|X)\n",
    "    return compute_entropy(headline_labels) - compute_entropy_for_specific_vocab(document_term_matrix, headline_labels, \n",
    "                          vectorized_vocabulary, specific_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',min_df=3,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_data, training_labels), (validation_data, validation_labels), (test_data, test_labels) = load_data(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with depth 1 and criteria = entropy : validation score = 0.6952965235173824\n",
      "Tree with depth 2 and criteria = entropy : validation score = 0.7157464212678937\n",
      "Tree with depth 3 and criteria = entropy : validation score = 0.7157464212678937\n",
      "Tree with depth 4 and criteria = entropy : validation score = 0.7157464212678937\n",
      "Tree with depth 5 and criteria = entropy : validation score = 0.7075664621676891\n",
      "Tree with depth 1 and criteria = gini : validation score = 0.6952965235173824\n",
      "Tree with depth 2 and criteria = gini : validation score = 0.7157464212678937\n",
      "Tree with depth 3 and criteria = gini : validation score = 0.7157464212678937\n",
      "Tree with depth 4 and criteria = gini : validation score = 0.7177914110429447\n",
      "Tree with depth 5 and criteria = gini : validation score = 0.7157464212678937\n",
      "\n",
      "Best model is:\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n",
      "\n",
      "Best score is:\n",
      "0.7177914110429447\n"
     ]
    }
   ],
   "source": [
    "best_model = select_model(training_data, training_labels, validation_data, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the hyperparameters which achieved the highest validation accuracy and visualize the layers of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_best_tree(best_model, \"Top_Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot](Top_Tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will compute information gain on the chosen specific vaocabulary words (borrowing some ideas from the visualized tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_labels_tuple = [] #initialize list of tuples (vocab, info_gain) for sorting purposes later\n",
    "for specific_vocab in [\"donald\", \"trump\", \"hillary\", \"clinton\", \"election\", \"putin\", \"china\", \"korea\", \"america\", \"trade\"]:\n",
    "    information_gain = compute_information_gain(training_data, training_labels, vectorizer.vocabulary_, specific_vocab)\n",
    "    word_labels_tuple.append((specific_vocab, information_gain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Information Gain by specific words in descending order: \n",
      "Information Gain By splitting on donald : 0.031762341558152274\n",
      "Information Gain By splitting on hillary : 0.026716218554747928\n",
      "Information Gain By splitting on trump : 0.02514069741856484\n",
      "Information Gain By splitting on korea : 0.012302477027561709\n",
      "Information Gain By splitting on clinton : 0.006625438294063257\n",
      "Information Gain By splitting on america : 0.005114136997096508\n",
      "Information Gain By splitting on trade : 0.002745505981260443\n",
      "Information Gain By splitting on china : 0.002140232566689937\n",
      "Information Gain By splitting on putin : 0.0018776484873886945\n",
      "Information Gain By splitting on election : 0.0004577864196121384\n"
     ]
    }
   ],
   "source": [
    "#Print and Sort Information Gain in descending order\n",
    "print(\"Sorted Information Gain by specific words in descending order: \")\n",
    "sorted_labels = sorted(word_labels_tuple,key=lambda x: x[1], reverse=True) \n",
    "for item in sorted_labels:\n",
    "    print(f\"Information Gain By splitting on \" + item[0] +\" : \" + str(item[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
